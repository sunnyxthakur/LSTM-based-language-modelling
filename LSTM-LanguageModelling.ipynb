{"cells": [{"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "import time\nimport numpy as np\nimport tensorflow as tf", "execution_count": 6, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "scrolled": true}, "cell_type": "code", "source": "!mkdir data\n!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n!unzip -o data/ptb.zip -d data\n!cp data/ptb/reader.py .\n\nimport reader", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "mkdir: cannot create directory \u2018data\u2019: File exists\nArchive:  data/ptb.zip\n  inflating: data/ptb/reader.py      \n  inflating: data/__MACOSX/ptb/._reader.py  \n  inflating: data/__MACOSX/._ptb     \n", "name": "stdout"}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<a id=\"building_lstm_model\"></a>\n<h2>Building the LSTM model for Language Modeling</h2>\n"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n!tar xzf simple-examples.tgz -C data/", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "--2020-06-19 07:06:29--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\nResolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\nConnecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 34869662 (33M) [application/x-gtar]\nSaving to: \u2018simple-examples.tgz.1\u2019\n\n100%[======================================>] 34,869,662  3.28MB/s   in 14s    \n\n2020-06-19 07:06:44 (2.39 MB/s) - \u2018simple-examples.tgz.1\u2019 saved [34869662/34869662]\n\n", "name": "stdout"}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "#Initial weight scale\ninit_scale = 0.1\n#Initial learning rate\nlearning_rate = 1.0\n#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\nmax_grad_norm = 5\n#The number of layers in our model\nnum_layers = 2\n#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\nnum_steps = 20\n#The number of processing units (neurons) in the hidden layers\nhidden_size_l1 = 256\nhidden_size_l2 = 128\n#The maximum number of epochs trained with the initial learning rate\nmax_epoch_decay_lr = 4\n#The total number of epochs in training\nmax_epoch = 15\n#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n#At 1, we ignore the Dropout Layer wrapping.\nkeep_prob = 1\n#The decay for the learning rate\ndecay = 0.5\n#The size for each batch of data\nbatch_size = 60\n#The size of our vocabulary\nvocab_size = 10000\nembeding_vector_size = 200\n#Training flag to separate training from testing\nis_training = 1\n#Data directory for our dataset\ndata_dir = \"data/simple-examples/data/\"", "execution_count": 18, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Network structure:\n<ul>\n    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n    </li>\n    <li>The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n    <li>the structure is like:\n        <ul>\n            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n        </ul>\n    </li>\n</ul>\n<br>\n\nInput layer: \n<ul>\n    <li>The network has 200 input units.</li>\n    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n    </li>\n    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n    </li>\n</ul>\n<br>\n\nHidden layer:\n<ul>\n    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n</ul>\n<br>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "\n<h4>Train data</h4>\n<ul>\n    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, andh is size of each sentence. So, the number of iterators is 1548\n    </li>\n    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>\n</ul>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session = tf.InteractiveSession()", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n", "name": "stderr"}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Reads the data and separates it into training data, validation data and testing data\nraw_data = reader.ptb_raw_data(data_dir)\ntrain_data, valid_data, test_data, vocab, word_to_id = raw_data", "execution_count": 22, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(train_data)", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "929589"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "len(valid_data)", "execution_count": 26, "outputs": [{"output_type": "execute_result", "execution_count": 26, "data": {"text/plain": "73760"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "def id_to_word(id_list):\n    line = []\n    for w in id_list:\n        for word, wid in word_to_id.items():\n            if wid == w:\n                line.append(word)\n    return line            \n                \n\nprint(id_to_word(train_data[0:100]))", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n", "name": "stdout"}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\nfirst_touple = itera.__next__()\nx = first_touple[0]\ny = first_touple[1]", "execution_count": 30, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "x.shape", "execution_count": 32, "outputs": [{"output_type": "execute_result", "execution_count": 32, "data": {"text/plain": "(60, 20)"}, "metadata": {}}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Lets look at 3 sentences of our input x:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "x[0:3]", "execution_count": 34, "outputs": [{"output_type": "execute_result", "execution_count": 34, "data": {"text/plain": "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n      dtype=int32)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "y[0:3]", "execution_count": 36, "outputs": [{"output_type": "execute_result", "execution_count": 36, "data": {"text/plain": "array([[9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n        9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996],\n       [  33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,    8,\n        2836,    2,  269,    4, 5526,  241,   13, 2420,    7],\n       [   6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,  123,\n           7,  514,    2,   63,   10,  514,    8,  605, 4886]],\n      dtype=int32)"}, "metadata": {}}]}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "we define 2 place holders to feed them with mini-batchs, that is x and y:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]", "execution_count": 38, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "feed_dict = {_input_data:x, _targets:y}", "execution_count": 40, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(_input_data, feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\nlstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Also, we initialize the states of the nework:\n\n<h4>_initial_state</h4>\n\nFor each LCTM, there are 2 state matrices, c\\_state and m\\_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n_initial_state", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Lets look at the states, though they are all zero for now:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(_initial_state, feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Lets initialize the <code>embedding_words</code> variable with random values."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "scrolled": true}, "cell_type": "code", "source": "session.run(tf.global_variables_initializer())\nsession.run(embedding_vocab)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\nIt creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Define where to get the data for our embeddings from\ninputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \ninputs", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(inputs[0], feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h3>Constructing Recurrent Neural Networks</h3>\n<b>tf.nn.dynamic_rnn()</b> creates a recurrent neural network using <b>stacked_lstm</b>. \n\nThe input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n\nThis method, returns a pair (outputs, new_state) where:\n<ul>\n    <li><b>outputs</b>: is a length T list of outputs (one for each input), or a nested tuple of such elements.</li>\n    <li><b>new_state</b>: is the final state.</li>\n</ul>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "scrolled": true}, "cell_type": "code", "source": "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "outputs", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(tf.global_variables_initializer())\nsession.run(outputs[0], feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "we need to flatten the outputs to be able to connect it softmax layer. Lets reshape the output tensor from  [30 x 20 x 200] to [600 x 200].\n\n<b>Notice:</b> Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself): \n<ul>\n    <li>sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]</li> \n    <li>sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]</li>   \n    <li>sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]</li>  \n    <li>...  </li>\n    <li>sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]</li>   \n</ul>\nNow, the flatten would convert this 3-dim tensor to:\n\n[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]\n"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "output = tf.reshape(outputs, [-1, hidden_size_l2])\noutput", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h3>logistic unit</h3>\nNow, we create a logistic unit to return the probability of the output word in our vocabulary with 1000 words. \n\n$$Softmax = [600 \\times 200] * [200 \\times 1000] + [1 \\times 1000] \\Longrightarrow [600 \\times 1000]$$"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\nsoftmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\nlogits = tf.matmul(output, softmax_w) + softmax_b\nprob = tf.nn.softmax(logits)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Lets look at the probability of observing words for t=0 to t=20:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(tf.global_variables_initializer())\noutput_words_prob = session.run(prob, feed_dict)\nprint(\"shape of the output: \", output_words_prob.shape)\nprint(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])\nprint(output_words_prob[0:20].shape)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h3>Prediction</h3>\nWhat is the word correspond to the probability output? Lets use the maximum probability:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "np.argmax(output_words_prob[0:20], axis=1)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "So, what is the ground truth for the first word of first sentence? "}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "y[0]", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Also, you can get it from target tensor, if you want to find the embedding vector:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "targ = session.run(_targets, feed_dict) \ntarg[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "How similar the predicted words are to the target words?"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h4>Objective function</h4>\n\nNow we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n\n$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n\nThis function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n\nThe arguments of this function are:  \n<ul>\n    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n    <li>weights: List of 1D batch-sized float-Tensors of the same length as logits.</li> \n</ul>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "loss is a 1D batch-sized float Tensor [600x1]: The log-perplexity for each sequence. Lets look at the first 10 values of loss:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(loss, feed_dict)[:10]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we define loss as average of the losses:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "cost = tf.reduce_sum(loss) / batch_size\nsession.run(tf.global_variables_initializer())\nsession.run(cost, feed_dict)\n", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h3>Training</h3>\n\nTo do training for our network, we have to take the following steps:\n<ol>\n    <li>Define the optimizer.</li>\n    <li>Extract variables that are trainable.</li>\n    <li>Calculate the gradients based on the loss function.</li>\n    <li>Apply the optimizer to the variables/gradients tuple.</li>\n</ol>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h4>1. Define Optimizer</h4>\n\n<b>GradientDescentOptimizer</b> constructs a new gradient descent optimizer. Later, we use constructed <b>optimizer</b> to compute gradients for a loss and apply gradients to variables."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Create a variable for the learning rate\nlr = tf.Variable(0.0, trainable=False)\n# Create the gradient descent optimizer with our learning rate\noptimizer = tf.train.GradientDescentOptimizer(lr)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "\n<h4>2. Trainable Variables</h4>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\ntvars = tf.trainable_variables()\ntvars", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Note: we can find the name and scope of all variables:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "[v.name for v in tvars]", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h4>3. Calculate the gradients based on the loss function</h4>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h4>Gradient</h4>:\nThe gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n\nFirst lets recall the gradient function using an toy example:\n$$ z = \\left(2x^2 + 3xy\\right)$$"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "The <b>tf.gradients()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors\u2014including variables. <b>tf.gradients(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w.r.t. <i>x</i> in <b>xs</b>. \n\nNow, lets look at the derivitive w.r.t. <b>var_x</b>:\n$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $$\n"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "the derivative w.r.t. <b>var_y</b>:\n$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 3x $$"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Now, we can look at gradients w.r.t all variables:"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "tf.gradients(cost, tvars)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "scrolled": true}, "cell_type": "code", "source": "grad_t_list = tf.gradients(cost, tvars)\nsession.run(grad_t_list,feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "\n\n\nnow, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.\n\n<b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:\n<ul>\n    <li>a list of clipped tensors, so called <i>list_clipped</i></li> \n    <li>the global norm (global_norm) of all tensors in t_list</li> \n</ul>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Define the gradient clipping threshold\ngrads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\ngrads", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(grads, feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "<h4>4. Apply the optimizer to the variables / gradients tuple.</h4>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Create the training TensorFlow Operation through our optimizer\ntrain_op = optimizer.apply_gradients(zip(grads, tvars))", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "session.run(tf.global_variables_initializer())\nsession.run(train_op, feed_dict)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"ltsm\"></a>\n<h2>LSTM</h2>"}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:\n<ul>\n    <li>We have to create the model in accordance with our defined hyperparameters</li>\n    <li>We have to create the placeholders for our input data and expected outputs (the real data)</li>\n    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n    <li>We have to create the word embeddings and point them to the input data</li>\n    <li>We have to create the input structure for our RNN</li>\n    <li>We have to instantiate our RNN model and retrieve the variable in which we should expect our outputs to appear</li>\n    <li>We need to create a logistic structure to return the probability of our words</li>\n    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n</ul>"}, {"metadata": {}, "cell_type": "code", "source": "hidden_size_l1", "execution_count": null, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "class PTBModel(object):\n\n    def __init__(self, action_type):\n        ######################################\n        # Setting parameters for ease of use #\n        ######################################\n        self.batch_size = batch_size\n        self.num_steps = num_steps\n        self.hidden_size_l1 = hidden_size_l1\n        self.hidden_size_l2 = hidden_size_l2\n        self.vocab_size = vocab_size\n        self.embeding_vector_size = embeding_vector_size\n        ###############################################################################\n        # Creating placeholders for our input data and expected outputs (target data) #\n        ###############################################################################\n        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n\n        ##########################################################################\n        # Creating the LSTM cell structure and connect it with the RNN structure #\n        ##########################################################################\n        # Create the LSTM unit. \n        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n        \n        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n        # This is an optimization of the LSTM output, but is not needed at all\n        if action_type == \"is_training\" and keep_prob < 1:\n            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n        \n        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n        # RNN cell composed sequentially of multiple simple cells.\n        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n\n        # Define the initial state, i.e., the model state for the very first data point\n        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n\n        ####################################################################\n        # Creating the word embeddings and pointing them to the input data #\n        ####################################################################\n        with tf.device(\"/cpu:0\"):\n            # Create the embeddings for our input data. Size is hidden size.\n            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n            # Define where to get the data for our embeddings from\n            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n\n        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n        # This is an optimization of the input processing and is not needed at all\n        if action_type == \"is_training\" and keep_prob < 1:\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        ############################################\n        # Creating the input structure for our RNN #\n        ############################################\n        # Input structure is 20x[30x200]\n        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n        # The input structure is fed from the embeddings, which are filled in by the input data\n        # Feeding a batch of b sentences to a RNN:\n        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n        # In step 2,  second word of each of the b sentences is input in parallel. \n        # The parallelism is only for efficiency.  \n        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n\n        ####################################################################################################\n        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n        ####################################################################################################\n        \n        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n        #########################################################################\n        # Creating a logistic unit to return the probability of the output word #\n        #########################################################################\n        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n        logits = tf.matmul(output, softmax_w) + softmax_b\n        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n        prob = tf.nn.softmax(logits)\n        out_words = tf.argmax(prob, axis=2)\n        self._output_words = out_words\n        #########################################################################\n        # Defining the loss and cost functions for the model's learning to work #\n        #########################################################################\n            \n\n        # Use the contrib sequence loss and average over the batches\n        loss = tf.contrib.seq2seq.sequence_loss(\n            logits,\n            self.targets,\n            tf.ones([batch_size, num_steps], dtype=tf.float32),\n            average_across_timesteps=False,\n            average_across_batch=True)\n    \n#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n#                                                       [tf.ones([batch_size * num_steps])])\n        self._cost = tf.reduce_sum(loss)\n\n        # Store the final state\n        self._final_state = state\n\n        #Everything after this point is relevant only for training\n        if action_type != \"is_training\":\n            return\n\n        #################################################\n        # Creating the Training Operation for our Model #\n        #################################################\n        # Create a variable for the learning rate\n        self._lr = tf.Variable(0.0, trainable=False)\n        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n        tvars = tf.trainable_variables()\n        # Define the gradient clipping threshold\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n        # Create the gradient descent optimizer with our learning rate\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n        # Create the training TensorFlow Operation through our optimizer\n        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    # Helper functions for our LSTM RNN class\n\n    # Assign the learning rate for this model\n    def assign_lr(self, session, lr_value):\n        session.run(tf.assign(self.lr, lr_value))\n\n    # Returns the input data for this model at a point in time\n    @property\n    def input_data(self):\n        return self._input_data\n\n\n    \n    # Returns the targets for this model at a point in time\n    @property\n    def targets(self):\n        return self._targets\n\n    # Returns the initial state for this model\n    @property\n    def initial_state(self):\n        return self._initial_state\n\n    # Returns the defined Cost\n    @property\n    def cost(self):\n        return self._cost\n\n    # Returns the final state for this model\n    @property\n    def final_state(self):\n        return self._final_state\n    \n    # Returns the final output words for this model\n    @property\n    def final_output_words(self):\n        return self._output_words\n    \n    # Returns the current learning rate for this model\n    @property\n    def lr(self):\n        return self._lr\n\n    # Returns the training operation defined for this model\n    @property\n    def train_op(self):\n        return self._train_op", "execution_count": 42, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n\nWhat our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "##########################################################################################################################\n# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n##########################################################################################################################\ndef run_one_epoch(session, m, data, eval_op, verbose=False):\n\n    #Define the epoch size based on the length of the data, batch size and the number of steps\n    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n\n    state = session.run(m.initial_state)\n    \n    #For each step and data point\n    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n        \n        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n                                     {m.input_data: x,\n                                      m.targets: y,\n                                      m.initial_state: state})\n\n        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n        costs += cost\n        \n        #Add number of steps to iteration counter\n        iters += m.num_steps\n\n        if verbose and step % (epoch_size // 10) == 10:\n            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n\n    # Returns the Perplexity rating for us to keep track of how the model is evolving\n    return np.exp(costs / iters)\n", "execution_count": 43, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "markdown", "source": "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data."}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Reads the data and separates it into training data, validation data and testing data\nraw_data = reader.ptb_raw_data(data_dir)\ntrain_data, valid_data, test_data, _, _ = raw_data", "execution_count": 44, "outputs": []}, {"metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}, "cell_type": "code", "source": "# Initializes the Execution Graph and the Session\nwith tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n    \n    # Instantiates the model for training\n    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n        m = PTBModel(\"is_training\")\n        \n    # Reuses the trained parameters for the validation and testing models\n    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n        mvalid = PTBModel(\"is_validating\")\n        mtest = PTBModel(\"is_testing\")\n\n    #Initialize all variables\n    tf.global_variables_initializer().run()\n\n    for i in range(max_epoch):\n        # Define the decay for this epoch\n        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n        \n        # Set the decayed learning rate as the learning rate for this epoch\n        m.assign_lr(session, learning_rate * lr_decay)\n\n        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n        \n        # Run the loop for this epoch in the training model\n        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n        \n        # Run the loop for this epoch in the validation model\n        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n    \n    # Run the loop in the testing model to see how effective was our training\n    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n    \n    print(\"Test Perplexity: %.3f\" % test_perplexity)", "execution_count": 45, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1 : Learning rate: 1.000\nItr 10 of 774, perplexity: 3661.921 speed: 1908 wps\nItr 87 of 774, perplexity: 1250.800 speed: 1335 wps\nItr 164 of 774, perplexity: 973.218 speed: 1327 wps\nItr 241 of 774, perplexity: 809.760 speed: 1301 wps\nItr 318 of 774, perplexity: 714.586 speed: 1292 wps\nItr 395 of 774, perplexity: 636.520 speed: 1283 wps\nItr 472 of 774, perplexity: 577.514 speed: 1289 wps\nItr 549 of 774, perplexity: 524.676 speed: 1298 wps\nItr 626 of 774, perplexity: 483.008 speed: 1299 wps\nItr 703 of 774, perplexity: 449.621 speed: 1292 wps\nEpoch 1 : Train Perplexity: 425.888\nEpoch 1 : Valid Perplexity: 261.013\nEpoch 2 : Learning rate: 1.000\nItr 10 of 774, perplexity: 275.464 speed: 1183 wps\nItr 87 of 774, perplexity: 238.210 speed: 1285 wps\nItr 164 of 774, perplexity: 228.504 speed: 1342 wps\nItr 241 of 774, perplexity: 218.743 speed: 1309 wps\nItr 318 of 774, perplexity: 216.176 speed: 1309 wps\nItr 395 of 774, perplexity: 210.338 speed: 1295 wps\nItr 472 of 774, perplexity: 205.985 speed: 1294 wps\nItr 549 of 774, perplexity: 199.270 speed: 1289 wps\nItr 626 of 774, perplexity: 193.720 speed: 1302 wps\nItr 703 of 774, perplexity: 189.552 speed: 1301 wps\nEpoch 2 : Train Perplexity: 186.804\nEpoch 2 : Valid Perplexity: 174.878\nEpoch 3 : Learning rate: 1.000\nItr 10 of 774, perplexity: 185.002 speed: 1637 wps\nItr 87 of 774, perplexity: 159.994 speed: 1314 wps\nItr 164 of 774, perplexity: 155.761 speed: 1361 wps\nItr 241 of 774, perplexity: 151.176 speed: 1353 wps\nItr 318 of 774, perplexity: 151.146 speed: 1331 wps\nItr 395 of 774, perplexity: 148.407 speed: 1317 wps\nItr 472 of 774, perplexity: 146.718 speed: 1335 wps\nItr 549 of 774, perplexity: 142.993 speed: 1325 wps\nItr 626 of 774, perplexity: 140.211 speed: 1329 wps\nItr 703 of 774, perplexity: 138.357 speed: 1327 wps\nEpoch 3 : Train Perplexity: 137.299\nEpoch 3 : Valid Perplexity: 151.406\nEpoch 4 : Learning rate: 1.000\nItr 10 of 774, perplexity: 147.655 speed: 2369 wps\nItr 87 of 774, perplexity: 126.955 speed: 1375 wps\nItr 164 of 774, perplexity: 124.712 speed: 1310 wps\nItr 241 of 774, perplexity: 121.442 speed: 1337 wps\nItr 318 of 774, perplexity: 122.074 speed: 1321 wps\nItr 395 of 774, perplexity: 120.192 speed: 1361 wps\nItr 472 of 774, perplexity: 119.327 speed: 1362 wps\nItr 549 of 774, perplexity: 116.524 speed: 1375 wps\nItr 626 of 774, perplexity: 114.637 speed: 1355 wps\nItr 703 of 774, perplexity: 113.530 speed: 1352 wps\nEpoch 4 : Train Perplexity: 112.982\nEpoch 4 : Valid Perplexity: 138.633\nEpoch 5 : Learning rate: 1.000\nItr 10 of 774, perplexity: 125.363 speed: 1045 wps\nItr 87 of 774, perplexity: 107.694 speed: 1422 wps\nItr 164 of 774, perplexity: 106.484 speed: 1383 wps\nItr 241 of 774, perplexity: 103.956 speed: 1406 wps\nItr 318 of 774, perplexity: 104.741 speed: 1335 wps\nItr 395 of 774, perplexity: 103.339 speed: 1363 wps\nItr 472 of 774, perplexity: 102.765 speed: 1356 wps\nItr 549 of 774, perplexity: 100.479 speed: 1351 wps\nItr 626 of 774, perplexity: 99.026 speed: 1333 wps\nItr 703 of 774, perplexity: 98.247 speed: 1325 wps\nEpoch 5 : Train Perplexity: 97.945\nEpoch 5 : Valid Perplexity: 134.277\nEpoch 6 : Learning rate: 0.500\nItr 10 of 774, perplexity: 107.600 speed: 1073 wps\nItr 87 of 774, perplexity: 91.965 speed: 1208 wps\nItr 164 of 774, perplexity: 90.093 speed: 1248 wps\nItr 241 of 774, perplexity: 87.358 speed: 1261 wps\nItr 318 of 774, perplexity: 87.577 speed: 1273 wps\nItr 395 of 774, perplexity: 85.832 speed: 1280 wps\nItr 472 of 774, perplexity: 84.984 speed: 1288 wps\nItr 549 of 774, perplexity: 82.631 speed: 1292 wps\nItr 626 of 774, perplexity: 81.010 speed: 1285 wps\nItr 703 of 774, perplexity: 79.957 speed: 1298 wps\nEpoch 6 : Train Perplexity: 79.357\nEpoch 6 : Valid Perplexity: 125.451\nEpoch 7 : Learning rate: 0.250\nItr 10 of 774, perplexity: 93.449 speed: 1269 wps\nItr 87 of 774, perplexity: 81.039 speed: 1303 wps\nItr 164 of 774, perplexity: 79.527 speed: 1336 wps\nItr 241 of 774, perplexity: 77.086 speed: 1367 wps\nItr 318 of 774, perplexity: 77.287 speed: 1370 wps\nItr 395 of 774, perplexity: 75.655 speed: 1337 wps\nItr 472 of 774, perplexity: 74.826 speed: 1331 wps\nItr 549 of 774, perplexity: 72.647 speed: 1350 wps\nItr 626 of 774, perplexity: 71.074 speed: 1355 wps\nItr 703 of 774, perplexity: 69.995 speed: 1349 wps\nEpoch 7 : Train Perplexity: 69.334\nEpoch 7 : Valid Perplexity: 123.807\nEpoch 8 : Learning rate: 0.125\nItr 10 of 774, perplexity: 85.840 speed: 1342 wps\nItr 87 of 774, perplexity: 74.953 speed: 1253 wps\nItr 164 of 774, perplexity: 73.673 speed: 1280 wps\nItr 241 of 774, perplexity: 71.390 speed: 1281 wps\nItr 318 of 774, perplexity: 71.609 speed: 1305 wps\nItr 395 of 774, perplexity: 70.068 speed: 1305 wps\nItr 472 of 774, perplexity: 69.262 speed: 1309 wps\nItr 549 of 774, perplexity: 67.197 speed: 1320 wps\nItr 626 of 774, perplexity: 65.685 speed: 1309 wps\nItr 703 of 774, perplexity: 64.621 speed: 1318 wps\nEpoch 8 : Train Perplexity: 63.950\nEpoch 8 : Valid Perplexity: 122.914\nEpoch 9 : Learning rate: 0.062\nItr 10 of 774, perplexity: 81.754 speed: 1235 wps\nItr 87 of 774, perplexity: 71.525 speed: 1258 wps\nItr 164 of 774, perplexity: 70.383 speed: 1324 wps\nItr 241 of 774, perplexity: 68.228 speed: 1319 wps\nItr 318 of 774, perplexity: 68.472 speed: 1309 wps\nItr 395 of 774, perplexity: 66.987 speed: 1330 wps\nItr 472 of 774, perplexity: 66.203 speed: 1314 wps\nItr 549 of 774, perplexity: 64.209 speed: 1310 wps\nItr 626 of 774, perplexity: 62.740 speed: 1325 wps\nItr 703 of 774, perplexity: 61.698 speed: 1301 wps\nEpoch 9 : Train Perplexity: 61.039\nEpoch 9 : Valid Perplexity: 122.204\nEpoch 10 : Learning rate: 0.031\nItr 10 of 774, perplexity: 79.677 speed: 1135 wps\nItr 87 of 774, perplexity: 69.658 speed: 1296 wps\nItr 164 of 774, perplexity: 68.574 speed: 1276 wps\nItr 241 of 774, perplexity: 66.497 speed: 1328 wps\nItr 318 of 774, perplexity: 66.755 speed: 1314 wps\nItr 395 of 774, perplexity: 65.300 speed: 1298 wps\nItr 472 of 774, perplexity: 64.528 speed: 1296 wps\nItr 549 of 774, perplexity: 62.576 speed: 1291 wps\nItr 626 of 774, perplexity: 61.128 speed: 1283 wps\nItr 703 of 774, perplexity: 60.103 speed: 1292 wps\nEpoch 10 : Train Perplexity: 59.454\nEpoch 10 : Valid Perplexity: 121.860\nEpoch 11 : Learning rate: 0.016\nItr 10 of 774, perplexity: 78.634 speed: 1122 wps\nItr 87 of 774, perplexity: 68.678 speed: 1332 wps\nItr 164 of 774, perplexity: 67.612 speed: 1333 wps\nItr 241 of 774, perplexity: 65.557 speed: 1355 wps\nItr 318 of 774, perplexity: 65.821 speed: 1332 wps\nItr 395 of 774, perplexity: 64.382 speed: 1314 wps\nItr 472 of 774, perplexity: 63.616 speed: 1316 wps\nItr 549 of 774, perplexity: 61.688 speed: 1299 wps\nItr 626 of 774, perplexity: 60.248 speed: 1300 wps\nItr 703 of 774, perplexity: 59.233 speed: 1288 wps\nEpoch 11 : Train Perplexity: 58.589\nEpoch 11 : Valid Perplexity: 121.706\nEpoch 12 : Learning rate: 0.008\nItr 10 of 774, perplexity: 78.038 speed: 1116 wps\nItr 87 of 774, perplexity: 68.141 speed: 1306 wps\nItr 164 of 774, perplexity: 67.088 speed: 1295 wps\nItr 241 of 774, perplexity: 65.038 speed: 1340 wps\nItr 318 of 774, perplexity: 65.303 speed: 1360 wps\nItr 395 of 774, perplexity: 63.875 speed: 1363 wps\nItr 472 of 774, perplexity: 63.113 speed: 1382 wps\nItr 549 of 774, perplexity: 61.198 speed: 1371 wps\nItr 626 of 774, perplexity: 59.764 speed: 1386 wps\nItr 703 of 774, perplexity: 58.754 speed: 1392 wps\nEpoch 12 : Train Perplexity: 58.112\nEpoch 12 : Valid Perplexity: 121.550\nEpoch 13 : Learning rate: 0.004\nItr 10 of 774, perplexity: 77.682 speed: 1008 wps\nItr 87 of 774, perplexity: 67.828 speed: 1432 wps\nItr 164 of 774, perplexity: 66.797 speed: 1376 wps\nItr 241 of 774, perplexity: 64.754 speed: 1385 wps\nItr 318 of 774, perplexity: 65.019 speed: 1359 wps\nItr 395 of 774, perplexity: 63.598 speed: 1333 wps\nItr 472 of 774, perplexity: 62.838 speed: 1341 wps\nItr 549 of 774, perplexity: 60.932 speed: 1329 wps\nItr 626 of 774, perplexity: 59.500 speed: 1325 wps\nItr 703 of 774, perplexity: 58.494 speed: 1326 wps\nEpoch 13 : Train Perplexity: 57.855\n", "name": "stdout"}, {"output_type": "stream", "text": "Epoch 13 : Valid Perplexity: 121.382\nEpoch 14 : Learning rate: 0.002\nItr 10 of 774, perplexity: 77.476 speed: 1040 wps\nItr 87 of 774, perplexity: 67.640 speed: 1322 wps\nItr 164 of 774, perplexity: 66.628 speed: 1398 wps\nItr 241 of 774, perplexity: 64.597 speed: 1376 wps\nItr 318 of 774, perplexity: 64.864 speed: 1363 wps\nItr 395 of 774, perplexity: 63.447 speed: 1353 wps\nItr 472 of 774, perplexity: 62.689 speed: 1362 wps\nItr 549 of 774, perplexity: 60.788 speed: 1355 wps\nItr 626 of 774, perplexity: 59.359 speed: 1358 wps\nItr 703 of 774, perplexity: 58.354 speed: 1359 wps\nEpoch 14 : Train Perplexity: 57.718\nEpoch 14 : Valid Perplexity: 121.241\nEpoch 15 : Learning rate: 0.001\nItr 10 of 774, perplexity: 77.361 speed: 2227 wps\nItr 87 of 774, perplexity: 67.530 speed: 1350 wps\nItr 164 of 774, perplexity: 66.526 speed: 1300 wps\nItr 241 of 774, perplexity: 64.505 speed: 1284 wps\nItr 318 of 774, perplexity: 64.777 speed: 1280 wps\nItr 395 of 774, perplexity: 63.364 speed: 1263 wps\nItr 472 of 774, perplexity: 62.607 speed: 1184 wps\nItr 549 of 774, perplexity: 60.709 speed: 1206 wps\nItr 626 of 774, perplexity: 59.282 speed: 1202 wps\nItr 703 of 774, perplexity: 58.279 speed: 1211 wps\nEpoch 15 : Train Perplexity: 57.644\nEpoch 15 : Valid Perplexity: 121.161\nTest Perplexity: 116.950\n", "name": "stdout"}]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "widgets": {"state": {}, "version": "1.1.2"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}